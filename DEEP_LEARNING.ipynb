{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/Fl9OBXUtZ5r+GNTA3DYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vansh7nvc/AI-ML/blob/main/DEEP_LEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DENSE LAYER (TENSERFLOW)"
      ],
      "metadata": {
        "id": "2ttPg1j8hX1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_42gimFBa5_v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_dim,output_dim):\n",
        "    super(MyDenseLayer,self).__init__()\n",
        "    self.W = self.add_weight([input_dim, output_dim])\n",
        "    self.b = self.add_weight([1, output_dim])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call(self,inputs):\n",
        "  z = tf.matmul(inputs, self.W)*sel.b\n",
        "\n",
        "  output = tf.math.sigmoid(z)\n",
        "  return output"
      ],
      "metadata": {
        "id": "7JbVRnIBeLq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DENSE LAYER (PYTORCH)"
      ],
      "metadata": {
        "id": "V1NCrbeMhf2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyDenseLayer(nn.Module):\n",
        "  def __init__(self, input_dim,output_dim):\n",
        "    super(MyDenseLayer,self).__init__()\n",
        "\n",
        "    self.W = nn.Parameter(torch.randn(input_dim, output_dim))\n",
        "    self.b = nn.Parameter(torch.randn(1, output_dim))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    z = torch.matmul(inputs, self.W)*self.b\n",
        "\n",
        "    output = torch.sigmoid(z)\n",
        "    return output"
      ],
      "metadata": {
        "id": "Gv3jW59VkINh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f3c7b66"
      },
      "source": [
        "## Detailed Explanation of a Dense Layer\n",
        "\n",
        "A **Dense Layer** (or **Fully Connected Layer**) is a layer where every neuron in the layer is connected to every neuron in the previous layer. It's one of the most common and basic types of layers in neural networks.\n",
        "\n",
        "Here's a breakdown of its components and how it works:\n",
        "\n",
        "1.  **Inputs:** The dense layer receives inputs from the previous layer (or the input data itself). These inputs are typically represented as a vector or a tensor.\n",
        "\n",
        "2.  **Weights (W):** Each connection between a neuron in the previous layer and a neuron in the dense layer has an associated weight. These weights are parameters that the neural network learns during training. The weights are typically organized in a matrix.\n",
        "\n",
        "3.  **Biases (b):** Each neuron in the dense layer has an associated bias term. Biases are also learnable parameters that allow the activation function to be shifted, providing more flexibility to the model. The biases are typically organized in a vector.\n",
        "\n",
        "4.  **Linear Transformation:** The core operation of a dense layer is a linear transformation of the inputs. This involves multiplying the input vector by the weight matrix and adding the bias vector. Mathematically, this can be represented as:\n",
        "\n",
        "    $$ z = inputs \\cdot W + b $$\n",
        "\n",
        "    where:\n",
        "    *   `inputs` is the input vector/tensor.\n",
        "    *   `W` is the weight matrix.\n",
        "    *   `b` is the bias vector.\n",
        "    *   `z` is the result of the linear transformation (often called the pre-activation or logit).\n",
        "\n",
        "5.  **Activation Function:** After the linear transformation, an activation function is typically applied to the result `z`. The activation function introduces non-linearity into the network, which is crucial for learning complex patterns. Common activation functions include:\n",
        "    *   **Sigmoid:** Squashes the output to a range between 0 and 1.\n",
        "    *   **ReLU (Rectified Linear Unit):** Outputs the input directly if it's positive, otherwise outputs zero.\n",
        "    *   **Tanh (Hyperbolic Tangent):** Squashes the output to a range between -1 and 1.\n",
        "\n",
        "    The output of the dense layer after applying the activation function is:\n",
        "\n",
        "    $$ output = activation(z) $$\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "A dense layer performs a linear transformation on its inputs using learned weights and biases, followed by a non-linear activation function. This process allows the network to learn complex relationships and make predictions based on the input data. Dense layers are often used as output layers for classification or regression tasks, or as hidden layers within deeper networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe2a65bf"
      },
      "source": [
        "## Detailed Explanation of a Deep Neural Network\n",
        "\n",
        "A **Deep Neural Network (DNN)** is a type of artificial neural network characterized by having multiple hidden layers between the input and output layers. The \"deep\" in deep learning refers to the depth of the network, meaning the number of layers.\n",
        "\n",
        "Here's a breakdown of the key concepts and how they work together:\n",
        "\n",
        "1.  **Layers:** A DNN is composed of multiple layers:\n",
        "    *   **Input Layer:** This is the first layer and receives the raw data. The number of neurons in the input layer is determined by the number of features in the input data.\n",
        "    *   **Hidden Layers:** These are the layers between the input and output layers. DNNs have one or more hidden layers. Each hidden layer typically consists of a number of neurons, and each neuron in a hidden layer is connected to all neurons in the previous and subsequent layers (in a fully connected network). The computations in these layers are not directly exposed to the outside world.\n",
        "    *   **Output Layer:** This is the final layer and produces the network's output. The number of neurons in the output layer depends on the task. For example, in a binary classification task, there might be one output neuron, while in a multi-class classification task, there might be one output neuron for each class.\n",
        "\n",
        "2.  **Neurons (Nodes):** Each layer is composed of neurons (also called nodes). Each neuron receives input from the neurons in the previous layer, performs a computation, and then passes the output to the neurons in the next layer. The computation within a neuron typically involves:\n",
        "    *   Taking a weighted sum of the inputs.\n",
        "    *   Adding a bias term.\n",
        "    *   Applying an activation function to the result.\n",
        "\n",
        "3.  **Weights and Biases:** As with dense layers, each connection between neurons has a weight, and each neuron has a bias. These weights and biases are the parameters that the network learns during training. The learning process involves adjusting these parameters to minimize the difference between the network's output and the desired output.\n",
        "\n",
        "4.  **Activation Functions:** Non-linear activation functions are applied to the output of each neuron (except possibly the output layer, depending on the task). These functions are crucial for allowing the network to learn complex, non-linear relationships in the data. Common activation functions include ReLU, sigmoid, and tanh.\n",
        "\n",
        "5.  **Forward Propagation:** This is the process of passing data through the network from the input layer to the output layer. The inputs are processed by each layer sequentially, with the output of one layer becoming the input to the next layer.\n",
        "\n",
        "6.  **Backward Propagation:** This is the process of training the network. It involves calculating the error between the network's output and the true output, and then using this error to adjust the weights and biases in the network. The error is propagated backward through the network, and optimization algorithms (like gradient descent) are used to update the parameters.\n",
        "\n",
        "**Why \"Deep\"?**\n",
        "\n",
        "The depth of a neural network allows it to learn increasingly complex and abstract representations of the input data. Each hidden layer can learn different levels of features. For example, in image recognition, early layers might learn simple features like edges and corners, while later layers might learn more complex features like shapes and objects. This hierarchical learning process enables DNNs to achieve state-of-the-art performance on many complex tasks.\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "Deep neural networks are used in a wide range of applications, including:\n",
        "\n",
        "*   Image and speech recognition\n",
        "*   Natural language processing\n",
        "*   Machine translation\n",
        "*   Object detection\n",
        "*   Autonomous vehicles\n",
        "*   Drug discovery\n",
        "\n",
        "In essence, a deep neural network is a powerful model that can learn complex patterns and relationships in data by processing it through multiple layers of non-linear transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loss** **optimization**"
      ],
      "metadata": {
        "id": "gu4KS2St4OYA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5P2tGwd4Ts6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}